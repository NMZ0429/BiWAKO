{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"0. Introduction This is the largest lake for many ready-to-use SOTA models. Models : Trained state-of-the-art models for various vision tasks in ONNXRuntime backend No-Code Modules : Easy interface to use those models for both prediction and visualizing output. No coding is needed. The interface is universal among all models in this library. Extentiability : Customizable modules to use it for applications such as realtime inference and web application. 1. Installation Install directly from this repository. $cd BiWAKO $pip install -e . Warning Downloading from pip server is currently suspended in order to simplify dependency requirements. We will update it soon. 2. Usage No matter which model you use, these interface is the same. import BiWAKO # 1. Initialize Model model = BiWAKO . MiDAS ( model = \"mono_depth_small\" ) # 2. Feed Image (accept cv2 image or path to the image) prediction = model . predict ( image_or_image_path ) # 3. Visiualize result as a cv2 image result_img = model . render ( prediction , image_or_image_path ) More specifically... Instantiate model with BiWAKO.ModelName(weight) . The ModelName and weight corresponding to the task you want to work on can be found at the table in the next section. Weight file is automaticaly downloaded. call predict(image) . image can be either path to the image or cv2 image array. call render(prediction, image) . prediction is the return value of predict() method and image is the same as above. Some model takes optional arguments to control details in the output. 2-2 High Level APIs We also provides some APIs to even accelerate productions. See API page for further details/ 4. Models The following list is the current availability of models with weight variations. Click the link at the model column for futher documentation. Task Model Weights Mono Depth Prediction MiDAS mono_depth_small mono_depth_large Salient Object Detection U2Net mobile basic human_seg portrait Super Resolution RealESRGAN super_resolution4864 super_resolution6464 Object Detection YOLO2/YOLO Please refer to docs for details Emotion Prediction FerPlus ferplus8 Human Parsing HumanParsing human_attribute Denoise HINet denoise_320_480 Face Detection YuNet yunet_120_160 Style Transfer AnimeGAN animeGAN512 Image Classification ResNetV2 resnet18v2 resnet50v2 resnet101v2 resnet152v2 Human Portrait Segmentation MODNet modnet_256 Semantic Segmentation FastSCNN fast_scnn384 fast_scnn7681344 Diver's View Segmentation SUIMNet suim_net_3248 suim_rsb_72128 suim_vgg_25632 suim_vgg_72128 5. Deployment It is extremely easy to use BiWAKO at application layer. 1. Real Time Prediction Any model can be used in the same way to run real-time inference. 2. FastAPI Implementation Like the above example, you can build simple Backend API for inference on web server. We have prepared sample deployment of the library with FastAPI. Read this for details . 3. Video Prediction We also provides pre-defined video prediction API. Read this for details","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#0-introduction","text":"This is the largest lake for many ready-to-use SOTA models. Models : Trained state-of-the-art models for various vision tasks in ONNXRuntime backend No-Code Modules : Easy interface to use those models for both prediction and visualizing output. No coding is needed. The interface is universal among all models in this library. Extentiability : Customizable modules to use it for applications such as realtime inference and web application.","title":"0. Introduction"},{"location":"#1-installation","text":"Install directly from this repository. $cd BiWAKO $pip install -e . Warning Downloading from pip server is currently suspended in order to simplify dependency requirements. We will update it soon.","title":"1. Installation"},{"location":"#2-usage","text":"No matter which model you use, these interface is the same. import BiWAKO # 1. Initialize Model model = BiWAKO . MiDAS ( model = \"mono_depth_small\" ) # 2. Feed Image (accept cv2 image or path to the image) prediction = model . predict ( image_or_image_path ) # 3. Visiualize result as a cv2 image result_img = model . render ( prediction , image_or_image_path ) More specifically... Instantiate model with BiWAKO.ModelName(weight) . The ModelName and weight corresponding to the task you want to work on can be found at the table in the next section. Weight file is automaticaly downloaded. call predict(image) . image can be either path to the image or cv2 image array. call render(prediction, image) . prediction is the return value of predict() method and image is the same as above. Some model takes optional arguments to control details in the output.","title":"2. Usage"},{"location":"#2-2-high-level-apis","text":"We also provides some APIs to even accelerate productions. See API page for further details/","title":"2-2 High Level APIs"},{"location":"#4-models","text":"The following list is the current availability of models with weight variations. Click the link at the model column for futher documentation. Task Model Weights Mono Depth Prediction MiDAS mono_depth_small mono_depth_large Salient Object Detection U2Net mobile basic human_seg portrait Super Resolution RealESRGAN super_resolution4864 super_resolution6464 Object Detection YOLO2/YOLO Please refer to docs for details Emotion Prediction FerPlus ferplus8 Human Parsing HumanParsing human_attribute Denoise HINet denoise_320_480 Face Detection YuNet yunet_120_160 Style Transfer AnimeGAN animeGAN512 Image Classification ResNetV2 resnet18v2 resnet50v2 resnet101v2 resnet152v2 Human Portrait Segmentation MODNet modnet_256 Semantic Segmentation FastSCNN fast_scnn384 fast_scnn7681344 Diver's View Segmentation SUIMNet suim_net_3248 suim_rsb_72128 suim_vgg_25632 suim_vgg_72128","title":"4. Models"},{"location":"#5-deployment","text":"It is extremely easy to use BiWAKO at application layer. 1. Real Time Prediction Any model can be used in the same way to run real-time inference. 2. FastAPI Implementation Like the above example, you can build simple Backend API for inference on web server. We have prepared sample deployment of the library with FastAPI. Read this for details . 3. Video Prediction We also provides pre-defined video prediction API. Read this for details","title":"5. Deployment"},{"location":"api/","text":"High Level APIs We currently offer Video Prediction : Predict entire video file and export visualized result as another video file.","title":"APIs"},{"location":"api/#high-level-apis","text":"We currently offer Video Prediction : Predict entire video file and export visualized result as another video file.","title":"High Level APIs"},{"location":"api/video_predictor/","text":"Video Predictor Easy way to run models on video inputs and export visualized results as videos. Usage from BiWAKO import MODNet from BiWAKO.api import VideoPredictor model = MODNet () video_predictor = VideoPredictor ( model ) video_predictor . run ( \"some/video.mp4\" , title = \"modnet_prediction.mp4\" ) BiWAKO.api.VideoPredictor __init__ ( self , model ) special Initiaslize VideoPredictor. Parameters: Name Type Description Default model BaseInference BiWAKO model instance. required Examples: >>> video_predictor = VideoPredictor ( model = BiWAKO . MiDAS ()) run ( self , video_path , title = None ) Predict video at video_path, render the result as a single mp4 and save the result at the same directory. Parameters: Name Type Description Default video_path str Path to the video to predict. required title str Title of the output mp4 file. Defaults to use the model name. None Examples: >>> video_predictor . run ( video_path = \"video.mp4\" )","title":"Video Prediction"},{"location":"api/video_predictor/#video-predictor","text":"Easy way to run models on video inputs and export visualized results as videos.","title":"Video Predictor"},{"location":"api/video_predictor/#usage","text":"from BiWAKO import MODNet from BiWAKO.api import VideoPredictor model = MODNet () video_predictor = VideoPredictor ( model ) video_predictor . run ( \"some/video.mp4\" , title = \"modnet_prediction.mp4\" )","title":"Usage"},{"location":"api/video_predictor/#biwakoapivideopredictor","text":"","title":"BiWAKO.api.VideoPredictor"},{"location":"api/video_predictor/#BiWAKO.api.video_predictor.VideoPredictor.__init__","text":"Initiaslize VideoPredictor. Parameters: Name Type Description Default model BaseInference BiWAKO model instance. required Examples: >>> video_predictor = VideoPredictor ( model = BiWAKO . MiDAS ())","title":"__init__()"},{"location":"api/video_predictor/#BiWAKO.api.video_predictor.VideoPredictor.run","text":"Predict video at video_path, render the result as a single mp4 and save the result at the same directory. Parameters: Name Type Description Default video_path str Path to the video to predict. required title str Title of the output mp4 file. Defaults to use the model name. None Examples: >>> video_predictor . run ( video_path = \"video.mp4\" )","title":"run()"},{"location":"demo/","text":"Demo 1. Asynchronous Inference Server with FastAPI First, run the backend inference server $python inference_server.py INFO: Started server process [ 18987 ] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 ( Press CTRL+C to quit ) Then, on the different process, run the front end web app via streamlit . Specify the address of the inference server you booted up above by -s option. For example, if the server is booted at http://127.0.0.1:8000 , pass 8000 . $streamlit run demo/webapp_demo.py -- -s 8000 2. Realtime Inference This requires a webcam connected to your computer. First, uncomment the mode you want to try and comment all others out. Following example chose MiDAS to try. model = BiWAKO . MiDAS ( \"weights/mono_depth_small\" ) # model = BiWAKO.U2Net(\"mobile\") # model = BiWAKO.HumanParsing(\"human_attribute\") Now, just run the script $python demo/live_demo.py 3. Video Inference Sample usage of VideoPredictor . Assign the path to input mp4 to video and run the script. $python demo/video_inference_demo.py","title":"Use Demo Scripts"},{"location":"demo/#demo","text":"","title":"Demo"},{"location":"demo/#1-asynchronous-inference-server-with-fastapi","text":"First, run the backend inference server $python inference_server.py INFO: Started server process [ 18987 ] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 ( Press CTRL+C to quit ) Then, on the different process, run the front end web app via streamlit . Specify the address of the inference server you booted up above by -s option. For example, if the server is booted at http://127.0.0.1:8000 , pass 8000 . $streamlit run demo/webapp_demo.py -- -s 8000","title":"1. Asynchronous Inference Server with FastAPI"},{"location":"demo/#2-realtime-inference","text":"This requires a webcam connected to your computer. First, uncomment the mode you want to try and comment all others out. Following example chose MiDAS to try. model = BiWAKO . MiDAS ( \"weights/mono_depth_small\" ) # model = BiWAKO.U2Net(\"mobile\") # model = BiWAKO.HumanParsing(\"human_attribute\") Now, just run the script $python demo/live_demo.py","title":"2. Realtime Inference"},{"location":"demo/#3-video-inference","text":"Sample usage of VideoPredictor . Assign the path to input mp4 to video and run the script. $python demo/video_inference_demo.py","title":"3. Video Inference"},{"location":"models/denoising/","text":"HINet Warning This is very large model and the weight file takes approximately 400MB on the disk. Original image and denoised image BiWAKO.HINet HINet for denoising image. Attributes: Name Type Description model InferenceSession ONNX model. input_name str Name of input node. input_shape tuple Shape of input node. __init__ ( self , model = 'denoise_320_480' ) special Initialize HINet. Parameters: Name Type Description Default model str Choise of model. Weight file is automatically downloaded to the current directory at the first time. Defaults to \"denoise_320_480.onnx\". 'denoise_320_480' predict ( self , image ) Return denoised image. Parameters: Name Type Description Default image Image Image to be denoised in str or cv2 format. required Returns: Type Description np.ndarray Denoised image array containing two images for different denoising methods. render ( self , prediction , image = None , output_type = 0 , output_shape = None ) Return the denoised image in original image size. Parameters: Name Type Description Default prediction np.ndarray Return value of predict(). required image Image Image to be processed in str or cv2 format. Defaults to None. None output_type Literal[0, 1] Choice of denoising method either 0 or 1. Defaults to 0. 0 output_shape Optional[Tuple[int, int]] Optional tuple of int to resize the return image. Defaults to None. None Exceptions: Type Description ValueError If none of the original image or image size is given. ValueError If output_type is not 0 or 1. Returns: Type Description np.ndarray Denoised image in cv2 format.","title":"Denoise Image"},{"location":"models/denoising/#hinet","text":"Warning This is very large model and the weight file takes approximately 400MB on the disk. Original image and denoised image","title":"HINet"},{"location":"models/denoising/#biwakohinet","text":"HINet for denoising image. Attributes: Name Type Description model InferenceSession ONNX model. input_name str Name of input node. input_shape tuple Shape of input node.","title":"BiWAKO.HINet"},{"location":"models/denoising/#BiWAKO.model.denoise.HINet.__init__","text":"Initialize HINet. Parameters: Name Type Description Default model str Choise of model. Weight file is automatically downloaded to the current directory at the first time. Defaults to \"denoise_320_480.onnx\". 'denoise_320_480'","title":"__init__()"},{"location":"models/denoising/#BiWAKO.model.denoise.HINet.predict","text":"Return denoised image. Parameters: Name Type Description Default image Image Image to be denoised in str or cv2 format. required Returns: Type Description np.ndarray Denoised image array containing two images for different denoising methods.","title":"predict()"},{"location":"models/denoising/#BiWAKO.model.denoise.HINet.render","text":"Return the denoised image in original image size. Parameters: Name Type Description Default prediction np.ndarray Return value of predict(). required image Image Image to be processed in str or cv2 format. Defaults to None. None output_type Literal[0, 1] Choice of denoising method either 0 or 1. Defaults to 0. 0 output_shape Optional[Tuple[int, int]] Optional tuple of int to resize the return image. Defaults to None. None Exceptions: Type Description ValueError If none of the original image or image size is given. ValueError If output_type is not 0 or 1. Returns: Type Description np.ndarray Denoised image in cv2 format.","title":"render()"},{"location":"models/emotion/","text":"Emotion Recognition Original image and denoised image BiWAKO.FerPlus Emotion prediction model. The model requires the input image to be trimmed around the face. Use YuNet to detect the face and crop the image around it. Attributes: Name Type Description model_path str Path to the model weights. If automatic download is triggered, this path is used to save the model. session onnxruntime.InferenceSession The inference session. input_name str The name of the input node. output_name str The name of the output node. emotion_table list A list of emotions trained. __init__ ( self , model = 'ferplus8' ) special Initialize the model. Parameters: Name Type Description Default model str The name of the model. Also accept the path to the onnx file. If not found, the model will be downloaded. Currently only support \"ferplus8\". 'ferplus8' predict ( self , image ) Return the array of the confidences of each predction. Parameters: Name Type Description Default image Image Image to be processed. Accept the path to the image or cv2 image. required Returns: Type Description np.ndarray The array of the confidences of each predction. render ( self , prediction , image , * args , ** kwargs ) Return the list of emotions and their confidences in string over the image. Parameters: Name Type Description Default prediction np.ndarray The array of the confidences of each predction. required image Image Image to be processed. Accept the path to the image or cv2 image. Not actually required. required Returns: Type Description np.ndarray Image with the list of emotions and their confidences in string.","title":"Emotion Recognition"},{"location":"models/emotion/#emotion-recognition","text":"Original image and denoised image","title":"Emotion Recognition"},{"location":"models/emotion/#biwakoferplus","text":"Emotion prediction model. The model requires the input image to be trimmed around the face. Use YuNet to detect the face and crop the image around it. Attributes: Name Type Description model_path str Path to the model weights. If automatic download is triggered, this path is used to save the model. session onnxruntime.InferenceSession The inference session. input_name str The name of the input node. output_name str The name of the output node. emotion_table list A list of emotions trained.","title":"BiWAKO.FerPlus"},{"location":"models/emotion/#BiWAKO.model.emotion_prediction.FerPlus.__init__","text":"Initialize the model. Parameters: Name Type Description Default model str The name of the model. Also accept the path to the onnx file. If not found, the model will be downloaded. Currently only support \"ferplus8\". 'ferplus8'","title":"__init__()"},{"location":"models/emotion/#BiWAKO.model.emotion_prediction.FerPlus.predict","text":"Return the array of the confidences of each predction. Parameters: Name Type Description Default image Image Image to be processed. Accept the path to the image or cv2 image. required Returns: Type Description np.ndarray The array of the confidences of each predction.","title":"predict()"},{"location":"models/emotion/#BiWAKO.model.emotion_prediction.FerPlus.render","text":"Return the list of emotions and their confidences in string over the image. Parameters: Name Type Description Default prediction np.ndarray The array of the confidences of each predction. required image Image Image to be processed. Accept the path to the image or cv2 image. Not actually required. required Returns: Type Description np.ndarray Image with the list of emotions and their confidences in string.","title":"render()"},{"location":"models/face_det/","text":"Face Detection Query image and prediction BiWAKO.YuNet Face Detection model. Attributes: Name Type Description model onnxruntime.InferenceSession ONNX model. input_name str name of input node. output_names list names of three output nodes. input_shape list shape of input image. Set to [160, 120] by default. conf_th float confidence threshold. Set to 0.6 by default. nms_th float non-maximum suppression threshold. Set to 0.3 by default. topk int keep top-k results. Set to 5000 by default. priors np.ndarray prior boxes. __init__ ( self , model = 'yunet_120_160' , input_shape = [ 160 , 120 ], conf_th = 0.6 , nms_th = 0.3 , topk = 5000 , keep_topk = 750 ) special Initialize YuNet. Parameters: Name Type Description Default model str model name. Set to \"yunet_120_160\" by default. 'yunet_120_160' input_shape list Input image shape. Defaults to [160, 120]. [160, 120] conf_th float Confidence level threshold. Defaults to 0.6. 0.6 nms_th float NMS threshold. Defaults to 0.3. 0.3 topk int Number of faces to detect. Defaults to 5000. 5000 keep_topk int Number of predictions to save. Defaults to 750. 750 predict ( self , image ) Return the face detection result. The prediction result is a tuple of three lists. First list is bounding boxes, second list is landmarks, and third list is scores. For example, accesing 2nd parson's bounding box is done by prediction [1][0] , and prediction [1][1] is landmarks of 2nd person. Parameters: Name Type Description Default image Image image to be detected. Accept path or cv2 image. required Returns: Type Description Tuple[list, list, list] Tuple of three lists of bounding box, landmark, and score. render ( self , prediction , image ) Render the bounding box and landmarks on the original image. Parameters: Name Type Description Default prediction tuple prediction result returned by predict(). required image Image original image in str or cv2 image. required Returns: Type Description np.ndarray Original image with bounding box and landmarks.","title":"Face Detection"},{"location":"models/face_det/#face-detection","text":"Query image and prediction","title":"Face Detection"},{"location":"models/face_det/#biwakoyunet","text":"Face Detection model. Attributes: Name Type Description model onnxruntime.InferenceSession ONNX model. input_name str name of input node. output_names list names of three output nodes. input_shape list shape of input image. Set to [160, 120] by default. conf_th float confidence threshold. Set to 0.6 by default. nms_th float non-maximum suppression threshold. Set to 0.3 by default. topk int keep top-k results. Set to 5000 by default. priors np.ndarray prior boxes.","title":"BiWAKO.YuNet"},{"location":"models/face_det/#BiWAKO.model.face_detection.YuNet.__init__","text":"Initialize YuNet. Parameters: Name Type Description Default model str model name. Set to \"yunet_120_160\" by default. 'yunet_120_160' input_shape list Input image shape. Defaults to [160, 120]. [160, 120] conf_th float Confidence level threshold. Defaults to 0.6. 0.6 nms_th float NMS threshold. Defaults to 0.3. 0.3 topk int Number of faces to detect. Defaults to 5000. 5000 keep_topk int Number of predictions to save. Defaults to 750. 750","title":"__init__()"},{"location":"models/face_det/#BiWAKO.model.face_detection.YuNet.predict","text":"Return the face detection result. The prediction result is a tuple of three lists. First list is bounding boxes, second list is landmarks, and third list is scores. For example, accesing 2nd parson's bounding box is done by prediction [1][0] , and prediction [1][1] is landmarks of 2nd person. Parameters: Name Type Description Default image Image image to be detected. Accept path or cv2 image. required Returns: Type Description Tuple[list, list, list] Tuple of three lists of bounding box, landmark, and score.","title":"predict()"},{"location":"models/face_det/#BiWAKO.model.face_detection.YuNet.render","text":"Render the bounding box and landmarks on the original image. Parameters: Name Type Description Default prediction tuple prediction result returned by predict(). required image Image original image in str or cv2 image. required Returns: Type Description np.ndarray Original image with bounding box and landmarks.","title":"render()"},{"location":"models/human_parsing/","text":"Human Parsing Query image and prediction BiWAKO.HumanParsing Basic ResNet50 model for parsing attributes of pedestrians Model This model is trained on the pedestrian dataset with following multi-label classification task: - \"is_male\", - \"has_bag\", - \"has_hat\", - \"has_longsleeves\", - \"has_longpants\", - \"has_longhair\", - \"has_coat_jacket\", Attributes: Name Type Description model onnxruntime.InferenceSession ONNXRuntime instance. conf_thresh float confidence threshold for prediction. input_name str name of input node. output_name str name of output node. input_shape tuple shape of input node. labels np.ndarray mapping of label index to label name. __init__ ( self , model = 'human_attribute' , conf_thresh = 0.4 ) special Initialize HumanParsing Parameters: Name Type Description Default model str model name or path to the onnx file. Defaults to \"human_attribute\". 'human_attribute' conf_thresh float confidence threshold. Defaults to 0.4. 0.4 predict ( self , image ) Return the prediction on the image. Parameters: Name Type Description Default image Image image to be processed in str or cv2 format. required Returns: Type Description np.ndarray processed image render ( self , prediction , image ) Return the original image with the prediction at the top left corner. Parameters: Name Type Description Default prediction np.ndarray prediction of the model. required image Image image to be rendered in str or cv2 format. required Returns: Type Description np.ndarray rendered image","title":"Human Parsing"},{"location":"models/human_parsing/#human-parsing","text":"Query image and prediction","title":"Human Parsing"},{"location":"models/human_parsing/#biwakohumanparsing","text":"Basic ResNet50 model for parsing attributes of pedestrians Model This model is trained on the pedestrian dataset with following multi-label classification task: - \"is_male\", - \"has_bag\", - \"has_hat\", - \"has_longsleeves\", - \"has_longpants\", - \"has_longhair\", - \"has_coat_jacket\", Attributes: Name Type Description model onnxruntime.InferenceSession ONNXRuntime instance. conf_thresh float confidence threshold for prediction. input_name str name of input node. output_name str name of output node. input_shape tuple shape of input node. labels np.ndarray mapping of label index to label name.","title":"BiWAKO.HumanParsing"},{"location":"models/human_parsing/#BiWAKO.model.human_attribute.HumanParsing.__init__","text":"Initialize HumanParsing Parameters: Name Type Description Default model str model name or path to the onnx file. Defaults to \"human_attribute\". 'human_attribute' conf_thresh float confidence threshold. Defaults to 0.4. 0.4","title":"__init__()"},{"location":"models/human_parsing/#BiWAKO.model.human_attribute.HumanParsing.predict","text":"Return the prediction on the image. Parameters: Name Type Description Default image Image image to be processed in str or cv2 format. required Returns: Type Description np.ndarray processed image","title":"predict()"},{"location":"models/human_parsing/#BiWAKO.model.human_attribute.HumanParsing.render","text":"Return the original image with the prediction at the top left corner. Parameters: Name Type Description Default prediction np.ndarray prediction of the model. required image Image image to be rendered in str or cv2 format. required Returns: Type Description np.ndarray rendered image","title":"render()"},{"location":"models/human_seg/","text":"Human Portrait Segmentation Query image, predicted segmentation map, and visualization BiWAKO.MODNet Segmentation model trained on human portrait image. Attributes: Name Type Description model onnxruntime.InferenceSession Inference session. input_name str Name of input node. output_name str Name of output node. input_shape tuple Size of input image. score_th float Threshold for mask. __init__ ( self , model = 'modnet_256' , score_th = 0.5 ) special Initialize MODNet. Parameters: Name Type Description Default model str Choice of model or path to the onnx file. Defaults to \"modnet_256\". If chosen model has not been downloaded, it will be downloaded automatically. 'modnet_256' score_th float Optional threshold for mask used in self.render(). Any pixels in the mask with confidence score lower than this value will be set to 0. Defaults to 0.5. 0.5 _preprocess ( self , image ) private Preprocess image for inference. This is automatically called by predict(). Preprocess Resize image to the same size as the model input. Normalize image to [-1, 1] with mean and std of 0.5. Convert image to float32 and reshape to (1, C, H, W). Parameters: Name Type Description Default image np.ndarray Image in cv2 format. required Returns: Type Description np.ndarray Preprocessed image in numpy format. predict ( self , image ) Return mask of given image. Parameters: Name Type Description Default image Image Image to be segmented. Accept path to image or cv2 image. required Returns: Type Description np.ndarray Predicted mask in original size. render ( self , prediction , image , black_out = False , score_th = None , ** kwargs ) Apply the mask to the input image. Parameters: Name Type Description Default prediction np.ndarray Mask returned by predict(). required image Image Image to be segmented. Accept path to image or cv2 image. required black_out bool Whether to use black background. Defaults to False. False score_th float Optional threshold for mask. Defaults to use the value set in the constructor. None Returns: Type Description np.ndarray Segmented image in cv2 format. Reference https://github.com/ZHKKKe/MODNet/blob/master/onnx/inference_onnx.py","title":"Human Portrait Segmentation"},{"location":"models/human_seg/#human-portrait-segmentation","text":"Query image, predicted segmentation map, and visualization","title":"Human Portrait Segmentation"},{"location":"models/human_seg/#biwakomodnet","text":"Segmentation model trained on human portrait image. Attributes: Name Type Description model onnxruntime.InferenceSession Inference session. input_name str Name of input node. output_name str Name of output node. input_shape tuple Size of input image. score_th float Threshold for mask.","title":"BiWAKO.MODNet"},{"location":"models/human_seg/#BiWAKO.model.human_seg.MODNet.__init__","text":"Initialize MODNet. Parameters: Name Type Description Default model str Choice of model or path to the onnx file. Defaults to \"modnet_256\". If chosen model has not been downloaded, it will be downloaded automatically. 'modnet_256' score_th float Optional threshold for mask used in self.render(). Any pixels in the mask with confidence score lower than this value will be set to 0. Defaults to 0.5. 0.5","title":"__init__()"},{"location":"models/human_seg/#BiWAKO.model.human_seg.MODNet._preprocess","text":"Preprocess image for inference. This is automatically called by predict(). Preprocess Resize image to the same size as the model input. Normalize image to [-1, 1] with mean and std of 0.5. Convert image to float32 and reshape to (1, C, H, W). Parameters: Name Type Description Default image np.ndarray Image in cv2 format. required Returns: Type Description np.ndarray Preprocessed image in numpy format.","title":"_preprocess()"},{"location":"models/human_seg/#BiWAKO.model.human_seg.MODNet.predict","text":"Return mask of given image. Parameters: Name Type Description Default image Image Image to be segmented. Accept path to image or cv2 image. required Returns: Type Description np.ndarray Predicted mask in original size.","title":"predict()"},{"location":"models/human_seg/#BiWAKO.model.human_seg.MODNet.render","text":"Apply the mask to the input image. Parameters: Name Type Description Default prediction np.ndarray Mask returned by predict(). required image Image Image to be segmented. Accept path to image or cv2 image. required black_out bool Whether to use black background. Defaults to False. False score_th float Optional threshold for mask. Defaults to use the value set in the constructor. None Returns: Type Description np.ndarray Segmented image in cv2 format.","title":"render()"},{"location":"models/human_seg/#reference","text":"https://github.com/ZHKKKe/MODNet/blob/master/onnx/inference_onnx.py","title":"Reference"},{"location":"models/image_clf/","text":"Image Classification Query image and prediction BiWAKO.ResNet Basic ResNet V2 trained on ImageNet Attributes: Name Type Description model_path str Path to the model file. If automatic download is not enabled, this path is used to save the file. model onnxruntime.InferenceSession Inference session for the model. input_name str Name of the input node. output_name str Name of the output node. input_shape tuple Shape of the input node. label dict Dictionary of the label. The key is the class index and the value is the class name. mean np.ndarray Mean of the normalization. var np.ndarray Variance of the normalization. __init__ ( self , model = 'resnet18v2' ) special Initialize ResNet Available models: \"resnet152v2\" \"resnet101v2\" \"resnet50v2\" or \"resnet18v2\" Parameters: Name Type Description Default model str Choice of the model from the table above or path to the downloaded onnx file. If the file has not been downloaded, the automatic download is triggered. Defaults to \"resnet18v2\". 'resnet18v2' predict ( self , image ) Return the prediction of the model Parameters: Name Type Description Default image Image Image to be predicted. Accept path or cv2 image. required Returns: Type Description np.ndarray 1 by 1000 array of the prediction. Softmax is not applied. render ( self , prediction , image , topk = 5 , ** kwargs ) Return the original image with the predicted class names Parameters: Name Type Description Default prediction np.ndarray Prediction returned by predict(). required image Image Image to be predicted. Accept path or cv2 image. required topk int Number of classes to display with higher probability. Defaults to 5. 5 Returns: Type Description np.ndarray Image with the predicted class names.","title":"Image Classification"},{"location":"models/image_clf/#image-classification","text":"Query image and prediction","title":"Image Classification"},{"location":"models/image_clf/#biwakoresnet","text":"Basic ResNet V2 trained on ImageNet Attributes: Name Type Description model_path str Path to the model file. If automatic download is not enabled, this path is used to save the file. model onnxruntime.InferenceSession Inference session for the model. input_name str Name of the input node. output_name str Name of the output node. input_shape tuple Shape of the input node. label dict Dictionary of the label. The key is the class index and the value is the class name. mean np.ndarray Mean of the normalization. var np.ndarray Variance of the normalization.","title":"BiWAKO.ResNet"},{"location":"models/image_clf/#BiWAKO.model.image_classification.ResNet.__init__","text":"Initialize ResNet Available models: \"resnet152v2\" \"resnet101v2\" \"resnet50v2\" or \"resnet18v2\" Parameters: Name Type Description Default model str Choice of the model from the table above or path to the downloaded onnx file. If the file has not been downloaded, the automatic download is triggered. Defaults to \"resnet18v2\". 'resnet18v2'","title":"__init__()"},{"location":"models/image_clf/#BiWAKO.model.image_classification.ResNet.predict","text":"Return the prediction of the model Parameters: Name Type Description Default image Image Image to be predicted. Accept path or cv2 image. required Returns: Type Description np.ndarray 1 by 1000 array of the prediction. Softmax is not applied.","title":"predict()"},{"location":"models/image_clf/#BiWAKO.model.image_classification.ResNet.render","text":"Return the original image with the predicted class names Parameters: Name Type Description Default prediction np.ndarray Prediction returned by predict(). required image Image Image to be predicted. Accept path or cv2 image. required topk int Number of classes to display with higher probability. Defaults to 5. 5 Returns: Type Description np.ndarray Image with the predicted class names.","title":"render()"},{"location":"models/mono_depth/","text":"Mono Depth Prediction Query image and prediction BiWAKO.MiDAS MonoDepth prediction model. Model This is a pretrained MiDASv1 model. Currently available models are: - mono_depth_small - mono_depth_large Attributes: Name Type Description model_path str Path to model file. If automatic download is triggered, this path is used to save the model. session onnxruntime.InferenceSession Inference session. input_name str Input node name. output_name str Output node name. input_shape tuple Input shape. h int Alias of input_shape[2]. w int Alias of input_shape[3]. __init__ ( self , model = 'mono_depth_small' , show_exp = False ) special Initialize model. Parameters: Name Type Description Default model str Model name or path to the downloaded onnx file. Defaults to \"mono_depth_small\". Onnx file is downloaded automatically. 'mono_depth_small' show_exp bool True to display expected input size. Defaults to False. False Examples: >>> model = MiDAS ( \"mono_depth_large\" ) downloading mono_depth_large . onnx to mono_depth_large . onnx 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 416 M / 416 M [ 02 : 11 < 00 : 00 , 3.47 MB / s ] >>> model = MiDAS ( \"weights/midas/mono_depth_small.onnx\" ) # download to a specific directory downloading mono_depth_small . onnx to weights / midas / mono_depth_small . onnx 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 66.8 M / 66.8 M [ 03 : 26 < 00 : 00 , 323 kB / s ] predict ( self , img ) Predict. Parameters: Name Type Description Default img Union[str, np.ndarray] image path or numpy array in cv2 format required Returns: Type Description np.ndarray predicted depthmap render ( self , prediction , query ) Return the resized depth map in cv2 foramt. Parameters: Name Type Description Default prediction np.ndarray predicted depthmap required query Union[str, np.ndarray] query image path or numpy array in cv2 format used for resizing. required Returns: Type Description np.ndarray Resized depthmap","title":"Mono Depth Prediction"},{"location":"models/mono_depth/#mono-depth-prediction","text":"Query image and prediction","title":"Mono Depth Prediction"},{"location":"models/mono_depth/#biwakomidas","text":"MonoDepth prediction model. Model This is a pretrained MiDASv1 model. Currently available models are: - mono_depth_small - mono_depth_large Attributes: Name Type Description model_path str Path to model file. If automatic download is triggered, this path is used to save the model. session onnxruntime.InferenceSession Inference session. input_name str Input node name. output_name str Output node name. input_shape tuple Input shape. h int Alias of input_shape[2]. w int Alias of input_shape[3].","title":"BiWAKO.MiDAS"},{"location":"models/mono_depth/#BiWAKO.model.mono_depth.MiDAS.__init__","text":"Initialize model. Parameters: Name Type Description Default model str Model name or path to the downloaded onnx file. Defaults to \"mono_depth_small\". Onnx file is downloaded automatically. 'mono_depth_small' show_exp bool True to display expected input size. Defaults to False. False Examples: >>> model = MiDAS ( \"mono_depth_large\" ) downloading mono_depth_large . onnx to mono_depth_large . onnx 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 416 M / 416 M [ 02 : 11 < 00 : 00 , 3.47 MB / s ] >>> model = MiDAS ( \"weights/midas/mono_depth_small.onnx\" ) # download to a specific directory downloading mono_depth_small . onnx to weights / midas / mono_depth_small . onnx 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 66.8 M / 66.8 M [ 03 : 26 < 00 : 00 , 323 kB / s ]","title":"__init__()"},{"location":"models/mono_depth/#BiWAKO.model.mono_depth.MiDAS.predict","text":"Predict. Parameters: Name Type Description Default img Union[str, np.ndarray] image path or numpy array in cv2 format required Returns: Type Description np.ndarray predicted depthmap","title":"predict()"},{"location":"models/mono_depth/#BiWAKO.model.mono_depth.MiDAS.render","text":"Return the resized depth map in cv2 foramt. Parameters: Name Type Description Default prediction np.ndarray predicted depthmap required query Union[str, np.ndarray] query image path or numpy array in cv2 format used for resizing. required Returns: Type Description np.ndarray Resized depthmap","title":"render()"},{"location":"models/obj_det/","text":"Object Detection Query image and prediction Currently, there are two YOLOv5 implementations avaialables for object detection task. Old implementation will be removed in future version. Those models contains a large number of pretrained weights. Please refer to the following table for available options. The first letter nano, s, ... is the scaling and suffix simp means the model is simplified by onnx simplifier. With the number 6 , the model is updated to the upstream version of YOLOv5. Model Weights Model Weights YOLO2 yolo_nano yolo_nano6 yolo_nano_simp yolo_nano6_simp yolo_s yolo_s6 yolo_s_simp yolo_s6_simp yolo_m yolo_m6 yolo_m_simp yolo_m6_simp yolo_l yolo_l_simp yolo_x yolo_x6 yolo_x_simp yolo_x6_simp YOLO yolo_nano yolo_s yolo_xl yolo_extreme yolo_nano_smp yolo_s_smp yolo_xl_smp yolo_extreme_smp BiWAKO.YOLO2 Note It is recommended to use this model rather than previous YOLO. This model optimizes pre/post-processing operations with new ort opsets. Runtime is 3~4 times faster than the previous model. If you want to use the raw output of the YOLO or customize post-processing with your choice of parameters, use the previous model below. Faster implementation of YOLO without any pre/post-processing. Attributes: Name Type Description model_path str The path to the onnx file. If automatic download is triggered, the file is downloaded to this path. session onnxruntime.InferenceSession The session of the onnx model. input_name str The name of the input node. coco_label List[str] The coco mapping of the labels. colors Colors Color palette written by Ultralytics at https://github.com/ultralytics/yolov5/blob/a3d5f1d3e36d8e023806da0f0c744eef02591c9b/utils/plots.py __init__ ( self , model = 'yolo_nano_simp' ) special Initialize YOLO2. Parameters: Name Type Description Default model str Choice of the model. Also accept the path to the downloaded onnx file. If the model has not been downloaded yet, the file is downloaded automatically. Defaults to \"yolo_nano_simp\". 'yolo_nano_simp' Examples: >>> model = YOLO2 ( \"yolo_nano_simp\" ) predict ( self , image ) Return the prediction of the model. Parameters: Name Type Description Default image Image The image to be predicted. Accept both path and array in cv2 format. required Returns: Type Description List[np.ndarray] The prediction of the model in the format of [scores, labels, boxes] . Examples: >>> model . predict ( \"path/to/image.jpg\" ) render ( self , prediction , image , ** kwargs ) Return the original image with the predicted bounding boxes. Parameters: Name Type Description Default prediction List[np.ndarray] The prediction of the model in the format of [scores, labels, boxes] . required image Image The image to be predicted. Accept both path and array in cv2 format. required Returns: Type Description np.ndarray The image with the predicted bounding boxes in cv2 format. Examples: >>> model . render ( model . predict ( \"path/to/image.jpg\" ), \"path/to/image.jpg\" ) BiWAKO.YOLO YOLOv5 onnx model. Attributes: Name Type Description model_path str Path to the onnx file. If auto download is triggered, the file is downloaded to this path. session onnxruntime.InferenceSession Inference session. input_name str Name of the input node. output_name str Name of the output node. input_shape tuple Shape of the input image. Set accordingly to the model. coco_label list List of coco 80 labels. colors Colors Color palette written by Ultralytics at https://github.com/ultralytics/yolov5/blob/a3d5f1d3e36d8e023806da0f0c744eef02591c9b/utils/plots.py __init__ ( self , model = 'yolo_nano' ) special Initialize the model. Parameters: Name Type Description Default model str Model type to be used. Also accept path to the onnx file. If the model is not found, it will be downloaded automatically. Currently [yolo_nano, yolo_s, yolo_xl and yolo_extreme] are supported. Default is yolo_nano . Adding _smp to the model name will use the simplified model. 'yolo_nano' Examples: >>> model = YOLO ( \"yolo_nano_smp\" ) downloading yolo_nano_smp . onnx to yolo_nano_smp . onnx 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 7.57 M / 7.57 M [ 00 : 01 < 00 : 00 , 6.89 MB / s ] predict ( self , image ) Return the prediction of the model. Parameters: Name Type Description Default image Image Image to be predicted. Accept str or cv2 image. required Returns: Type Description np.ndarray n by 6 array where 2nd dimension is xyxy with label and confidence. render ( self , prediction , image ) Return the original image with predicted bounding boxes. Parameters: Name Type Description Default prediction np.ndarray Prediction of the model. required image Image Image to be predicted. Accept str or cv2 image. required Returns: Type Description np.ndarray Image with predicted bounding boxes in cv2 format.","title":"Object Detection"},{"location":"models/obj_det/#object-detection","text":"Query image and prediction Currently, there are two YOLOv5 implementations avaialables for object detection task. Old implementation will be removed in future version. Those models contains a large number of pretrained weights. Please refer to the following table for available options. The first letter nano, s, ... is the scaling and suffix simp means the model is simplified by onnx simplifier. With the number 6 , the model is updated to the upstream version of YOLOv5. Model Weights Model Weights YOLO2 yolo_nano yolo_nano6 yolo_nano_simp yolo_nano6_simp yolo_s yolo_s6 yolo_s_simp yolo_s6_simp yolo_m yolo_m6 yolo_m_simp yolo_m6_simp yolo_l yolo_l_simp yolo_x yolo_x6 yolo_x_simp yolo_x6_simp YOLO yolo_nano yolo_s yolo_xl yolo_extreme yolo_nano_smp yolo_s_smp yolo_xl_smp yolo_extreme_smp","title":"Object Detection"},{"location":"models/obj_det/#biwakoyolo2","text":"Note It is recommended to use this model rather than previous YOLO. This model optimizes pre/post-processing operations with new ort opsets. Runtime is 3~4 times faster than the previous model. If you want to use the raw output of the YOLO or customize post-processing with your choice of parameters, use the previous model below. Faster implementation of YOLO without any pre/post-processing. Attributes: Name Type Description model_path str The path to the onnx file. If automatic download is triggered, the file is downloaded to this path. session onnxruntime.InferenceSession The session of the onnx model. input_name str The name of the input node. coco_label List[str] The coco mapping of the labels. colors Colors Color palette written by Ultralytics at https://github.com/ultralytics/yolov5/blob/a3d5f1d3e36d8e023806da0f0c744eef02591c9b/utils/plots.py","title":"BiWAKO.YOLO2"},{"location":"models/obj_det/#BiWAKO.model.yolo_refined.YOLO2.__init__","text":"Initialize YOLO2. Parameters: Name Type Description Default model str Choice of the model. Also accept the path to the downloaded onnx file. If the model has not been downloaded yet, the file is downloaded automatically. Defaults to \"yolo_nano_simp\". 'yolo_nano_simp' Examples: >>> model = YOLO2 ( \"yolo_nano_simp\" )","title":"__init__()"},{"location":"models/obj_det/#BiWAKO.model.yolo_refined.YOLO2.predict","text":"Return the prediction of the model. Parameters: Name Type Description Default image Image The image to be predicted. Accept both path and array in cv2 format. required Returns: Type Description List[np.ndarray] The prediction of the model in the format of [scores, labels, boxes] . Examples: >>> model . predict ( \"path/to/image.jpg\" )","title":"predict()"},{"location":"models/obj_det/#BiWAKO.model.yolo_refined.YOLO2.render","text":"Return the original image with the predicted bounding boxes. Parameters: Name Type Description Default prediction List[np.ndarray] The prediction of the model in the format of [scores, labels, boxes] . required image Image The image to be predicted. Accept both path and array in cv2 format. required Returns: Type Description np.ndarray The image with the predicted bounding boxes in cv2 format. Examples: >>> model . render ( model . predict ( \"path/to/image.jpg\" ), \"path/to/image.jpg\" )","title":"render()"},{"location":"models/obj_det/#biwakoyolo","text":"YOLOv5 onnx model. Attributes: Name Type Description model_path str Path to the onnx file. If auto download is triggered, the file is downloaded to this path. session onnxruntime.InferenceSession Inference session. input_name str Name of the input node. output_name str Name of the output node. input_shape tuple Shape of the input image. Set accordingly to the model. coco_label list List of coco 80 labels. colors Colors Color palette written by Ultralytics at https://github.com/ultralytics/yolov5/blob/a3d5f1d3e36d8e023806da0f0c744eef02591c9b/utils/plots.py","title":"BiWAKO.YOLO"},{"location":"models/obj_det/#BiWAKO.model.object_detection.YOLO.__init__","text":"Initialize the model. Parameters: Name Type Description Default model str Model type to be used. Also accept path to the onnx file. If the model is not found, it will be downloaded automatically. Currently [yolo_nano, yolo_s, yolo_xl and yolo_extreme] are supported. Default is yolo_nano . Adding _smp to the model name will use the simplified model. 'yolo_nano' Examples: >>> model = YOLO ( \"yolo_nano_smp\" ) downloading yolo_nano_smp . onnx to yolo_nano_smp . onnx 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 7.57 M / 7.57 M [ 00 : 01 < 00 : 00 , 6.89 MB / s ]","title":"__init__()"},{"location":"models/obj_det/#BiWAKO.model.object_detection.YOLO.predict","text":"Return the prediction of the model. Parameters: Name Type Description Default image Image Image to be predicted. Accept str or cv2 image. required Returns: Type Description np.ndarray n by 6 array where 2nd dimension is xyxy with label and confidence.","title":"predict()"},{"location":"models/obj_det/#BiWAKO.model.object_detection.YOLO.render","text":"Return the original image with predicted bounding boxes. Parameters: Name Type Description Default prediction np.ndarray Prediction of the model. required image Image Image to be predicted. Accept str or cv2 image. required Returns: Type Description np.ndarray Image with predicted bounding boxes in cv2 format.","title":"render()"},{"location":"models/salient_det/","text":"U2Net Query image and prediction BiWAKO.U2Net Salient object segmentation model. Attributes: Name Type Description IS onnxruntime.InferenceSession Inference session. input_name str Input node name. output_name str Output node name. input_size int Input size. Set to 320. mean List[float] Mean. Set to [0.485, 0.456, 0.406]. std List[float] Standard deviation. Set to [0.229, 0.224, 0.225]. __init__ ( self , model = 'mobile' ) special U2Net Inference class. Parameters: Name Type Description Default model str Model name or downloaded onnx file. Accept one of [\"basic\", \"mobile\", \"human_seg\", \"portrait\"] . If model has not been downloaded, it will be downloaded automatically. 'mobile' predict ( self , image ) Return the predicted mask of the image. Parameters: Name Type Description Default image Union[str, np.ndarray] Image in cv2 format or path to image. required Returns: Type Description np.ndarray Predicted mask in cv2 format. render ( self , prediction , image ) Apply the predicted mask to the original image. Parameters: Name Type Description Default prediction np.ndarray Predicted mask in cv2 format. required image Union[str, np.ndarray] Image in cv2 format or path to image. required Returns: Type Description np.ndarray Rendered original image in cv2 format.","title":"Salient Object Segmenation"},{"location":"models/salient_det/#u2net","text":"Query image and prediction","title":"U2Net"},{"location":"models/salient_det/#biwakou2net","text":"Salient object segmentation model. Attributes: Name Type Description IS onnxruntime.InferenceSession Inference session. input_name str Input node name. output_name str Output node name. input_size int Input size. Set to 320. mean List[float] Mean. Set to [0.485, 0.456, 0.406]. std List[float] Standard deviation. Set to [0.229, 0.224, 0.225].","title":"BiWAKO.U2Net"},{"location":"models/salient_det/#BiWAKO.model.segmentation.U2Net.__init__","text":"U2Net Inference class. Parameters: Name Type Description Default model str Model name or downloaded onnx file. Accept one of [\"basic\", \"mobile\", \"human_seg\", \"portrait\"] . If model has not been downloaded, it will be downloaded automatically. 'mobile'","title":"__init__()"},{"location":"models/salient_det/#BiWAKO.model.segmentation.U2Net.predict","text":"Return the predicted mask of the image. Parameters: Name Type Description Default image Union[str, np.ndarray] Image in cv2 format or path to image. required Returns: Type Description np.ndarray Predicted mask in cv2 format.","title":"predict()"},{"location":"models/salient_det/#BiWAKO.model.segmentation.U2Net.render","text":"Apply the predicted mask to the original image. Parameters: Name Type Description Default prediction np.ndarray Predicted mask in cv2 format. required image Union[str, np.ndarray] Image in cv2 format or path to image. required Returns: Type Description np.ndarray Rendered original image in cv2 format.","title":"render()"},{"location":"models/semantic_seg/","text":"Semantic Segmentation Very light weight semantic segmentation model. Query image and prediction BiWAKO.FastSCNN Semantic Segmentation Attributes: Name Type Description model_path str Path to the model file. model onnxruntime.InferenceSession Inference session. input_shape tuple Input shape of the model. Set to (384, 384) for fast_scnn384 and (1344, 768) for fast_scnn7681344. input_name str Name of the input node. output_name str Name of the output node. mean np.ndarray Mean value of the dataset. std np.ndarray Standard deviation of the dataset. c_map Color map for the semantic segmentation 19 object classes. __init__ ( self , model = 'fast_scnn384' , ** kwargs ) special Initialize FastSCNN model. Parameters: Name Type Description Default model str Choice of model. Accept model name or path to the downloaded onnx file. If onnx file has not been downloaded, it will be downloaded automatically. Currently avaiable models are [fast_scnn384, fast_scnn7681344] . Defaults to \"fast_scnn384\". 'fast_scnn384' _preprocess ( self , image ) private Preprocess the image. Automatically called. Preprocess Resize to the input shape. To RGB and normalize. Add the mean and divide by the standard deviation. Channel swap and float32. Add batch dimension. Parameters: Name Type Description Default image np.ndarray Image to be preprocessed. required Returns: Type Description np.ndarray Preprocessed image. predict ( self , image ) Return the prediction map. The last channel has 19 classes. Parameters: Name Type Description Default image Image Image to be predicted. Accept path to the image or cv2 image. required Returns: Type Description np.ndarray Prediction map in the shape of (height, width, 19). render ( self , prediction , image , ** kwargs ) Apply the prediction map to the image. Parameters: Name Type Description Default prediction np.ndarray Prediction map retuned by predict . required image Image Image to be rendered. Accept path to the image or cv2 image. required Returns: Type Description np.ndarray Rendered image.","title":"Semantic Segmentation"},{"location":"models/semantic_seg/#semantic-segmentation","text":"Very light weight semantic segmentation model. Query image and prediction","title":"Semantic Segmentation"},{"location":"models/semantic_seg/#biwakofastscnn","text":"Semantic Segmentation Attributes: Name Type Description model_path str Path to the model file. model onnxruntime.InferenceSession Inference session. input_shape tuple Input shape of the model. Set to (384, 384) for fast_scnn384 and (1344, 768) for fast_scnn7681344. input_name str Name of the input node. output_name str Name of the output node. mean np.ndarray Mean value of the dataset. std np.ndarray Standard deviation of the dataset. c_map Color map for the semantic segmentation 19 object classes.","title":"BiWAKO.FastSCNN"},{"location":"models/semantic_seg/#BiWAKO.model.semantic_segmentation.FastSCNN.__init__","text":"Initialize FastSCNN model. Parameters: Name Type Description Default model str Choice of model. Accept model name or path to the downloaded onnx file. If onnx file has not been downloaded, it will be downloaded automatically. Currently avaiable models are [fast_scnn384, fast_scnn7681344] . Defaults to \"fast_scnn384\". 'fast_scnn384'","title":"__init__()"},{"location":"models/semantic_seg/#BiWAKO.model.semantic_segmentation.FastSCNN._preprocess","text":"Preprocess the image. Automatically called. Preprocess Resize to the input shape. To RGB and normalize. Add the mean and divide by the standard deviation. Channel swap and float32. Add batch dimension. Parameters: Name Type Description Default image np.ndarray Image to be preprocessed. required Returns: Type Description np.ndarray Preprocessed image.","title":"_preprocess()"},{"location":"models/semantic_seg/#BiWAKO.model.semantic_segmentation.FastSCNN.predict","text":"Return the prediction map. The last channel has 19 classes. Parameters: Name Type Description Default image Image Image to be predicted. Accept path to the image or cv2 image. required Returns: Type Description np.ndarray Prediction map in the shape of (height, width, 19).","title":"predict()"},{"location":"models/semantic_seg/#BiWAKO.model.semantic_segmentation.FastSCNN.render","text":"Apply the prediction map to the image. Parameters: Name Type Description Default prediction np.ndarray Prediction map retuned by predict . required image Image Image to be rendered. Accept path to the image or cv2 image. required Returns: Type Description np.ndarray Rendered image.","title":"render()"},{"location":"models/style_transfer/","text":"AnimeGAN2 Transfer real image to Japanese anime style. Query image and prediction BiWAKO.AnimeGAN Style Transfer GAN trained for Anime. Attributes: Name Type Description model_path str Path to ONNX model file. If the file is automatically downloaded, the destination path is saved to this. model InferenceSession ONNX model. input_name str Name of input node. output_name str Name of output node. input_size int Size of input image. Set to 512. __init__ ( self , model = 'animeGAN512' ) special Initialize AnimeGAN model. Parameters: Name Type Description Default model str Either path to the downloaded model or name of the model to trigger automatic download. Defaults to \"animeGAN512\". 'animeGAN512' predict ( self , image ) Return the predicted image from the AnimeGAN model. Parameters: Name Type Description Default image Image Image to predict in str or cv2 image format. required Returns: Type Description np.ndarray Predicted image of size 512*512 in cv2 image format render ( self , prediction , image = None , input_size = None , ** kwargs ) Return the predicted image in original size. Parameters: Name Type Description Default prediction np.ndarray Predicted image of size 512*512 in cv2 image format. required image Image Original image passed to predict(). Defaults to None. None input_size Tuple[int, int] Optional tuple of int to resize. Defaults to None. None Exceptions: Type Description ValueError If none of image or input_size is provided. Returns: Type Description np.ndarray Predicted image in original size.","title":"Style Transfer"},{"location":"models/style_transfer/#animegan2","text":"Transfer real image to Japanese anime style. Query image and prediction","title":"AnimeGAN2"},{"location":"models/style_transfer/#biwakoanimegan","text":"Style Transfer GAN trained for Anime. Attributes: Name Type Description model_path str Path to ONNX model file. If the file is automatically downloaded, the destination path is saved to this. model InferenceSession ONNX model. input_name str Name of input node. output_name str Name of output node. input_size int Size of input image. Set to 512.","title":"BiWAKO.AnimeGAN"},{"location":"models/style_transfer/#BiWAKO.model.anime_gan.AnimeGAN.__init__","text":"Initialize AnimeGAN model. Parameters: Name Type Description Default model str Either path to the downloaded model or name of the model to trigger automatic download. Defaults to \"animeGAN512\". 'animeGAN512'","title":"__init__()"},{"location":"models/style_transfer/#BiWAKO.model.anime_gan.AnimeGAN.predict","text":"Return the predicted image from the AnimeGAN model. Parameters: Name Type Description Default image Image Image to predict in str or cv2 image format. required Returns: Type Description np.ndarray Predicted image of size 512*512 in cv2 image format","title":"predict()"},{"location":"models/style_transfer/#BiWAKO.model.anime_gan.AnimeGAN.render","text":"Return the predicted image in original size. Parameters: Name Type Description Default prediction np.ndarray Predicted image of size 512*512 in cv2 image format. required image Image Original image passed to predict(). Defaults to None. None input_size Tuple[int, int] Optional tuple of int to resize. Defaults to None. None Exceptions: Type Description ValueError If none of image or input_size is provided. Returns: Type Description np.ndarray Predicted image in original size.","title":"render()"},{"location":"models/suim_net/","text":"Diver's View Segmentation Query image and prediction BiWAKO.SUIMNet Semantic segmentation model for diver's view dataset. Attributes: Name Type Description model_path str Path to the onnx file. If automatic download is triggered, it is downloaded to this path. session onnxruntime.InferenceSession Inference session. h int Height of the input image. w int Width of the input image. input_name str Name of the input node. output_name str Name of the output node. c_map Color map for the segmentation. __init__ ( self , model = 'suim_rsb_72128' , num_classes = 5 , ** kwargs ) special Initialize the SUIMNet model. Parameters: Name Type Description Default model str Choice of the model or path to the downloaded onnx file. If the model hasn't been downloaded, it is automatically downloaded to this path. Defaults to \"suim_rsb_72128\". 'suim_rsb_72128' num_classes int Number of classes to segmentate. Defaults to 5. 5 predict ( self , image ) Return the segmentation map of the image. Parameters: Name Type Description Default image Image Image to segmentate. Accept path to the image or cv2 image. required Returns: Type Description np.ndarray Segmentation map of shape (h, w, num_classes). Each element is a confidence of the class. render ( self , prediction , image , ** kwargs ) Apply the segmentation map to the image. Parameters: Name Type Description Default prediction np.ndarray Segmentation map of shape (h, w, num_classes) returned by predict(). required image Image Input image. Accept path to the image or cv2 image. required Returns: Type Description np.ndarray Rendered image in cv2 format.","title":"Diver's View Segmentation"},{"location":"models/suim_net/#divers-view-segmentation","text":"Query image and prediction","title":"Diver's View Segmentation"},{"location":"models/suim_net/#biwakosuimnet","text":"Semantic segmentation model for diver's view dataset. Attributes: Name Type Description model_path str Path to the onnx file. If automatic download is triggered, it is downloaded to this path. session onnxruntime.InferenceSession Inference session. h int Height of the input image. w int Width of the input image. input_name str Name of the input node. output_name str Name of the output node. c_map Color map for the segmentation.","title":"BiWAKO.SUIMNet"},{"location":"models/suim_net/#BiWAKO.model.suim_net.SUIMNet.__init__","text":"Initialize the SUIMNet model. Parameters: Name Type Description Default model str Choice of the model or path to the downloaded onnx file. If the model hasn't been downloaded, it is automatically downloaded to this path. Defaults to \"suim_rsb_72128\". 'suim_rsb_72128' num_classes int Number of classes to segmentate. Defaults to 5. 5","title":"__init__()"},{"location":"models/suim_net/#BiWAKO.model.suim_net.SUIMNet.predict","text":"Return the segmentation map of the image. Parameters: Name Type Description Default image Image Image to segmentate. Accept path to the image or cv2 image. required Returns: Type Description np.ndarray Segmentation map of shape (h, w, num_classes). Each element is a confidence of the class.","title":"predict()"},{"location":"models/suim_net/#BiWAKO.model.suim_net.SUIMNet.render","text":"Apply the segmentation map to the image. Parameters: Name Type Description Default prediction np.ndarray Segmentation map of shape (h, w, num_classes) returned by predict(). required image Image Input image. Accept path to the image or cv2 image. required Returns: Type Description np.ndarray Rendered image in cv2 format.","title":"render()"},{"location":"models/super_resolution/","text":"Super Resolution Original image and upscaled image Warning Upscaling takes very long time (approximately 45 seconds per image). BiWAKO.RealESRGAN Super Resolution model. Attributes: Name Type Description model_path str Path to the model. If automatic download is enabled, it will be downloaded to this path. session rt.InferenceSession ONNX Runtime session. w, h (int Width and height of the model input. input_name str Name of the input node. output_name str Name of the output node. __init__ ( self , model = 'super_resolution4864' ) special Initialize RealESRGAN. Parameters: Name Type Description Default model str Model name or path to the downloaded onnx file. If model has not been downloaded, it will be downloaded automatically. Currently supports [\"super_resolution4864\", \"super_resolution6464\"]. 'super_resolution4864' predict ( self , image ) Return the upscaled image. Parameters: Name Type Description Default image Image Image to be upscaled. Accept path or cv2 image. required Returns: Type Description np.ndarray Upscaled image in cv2 format. render ( self , prediction , image ) Return the upscaled image. This is just a placeholder. Parameters: Name Type Description Default prediction np.ndarray Upscaled image in cv2 format. This image is returned. required image Image Original image. Not used. required Returns: Type Description np.ndarray Upscaled image in cv2 format.","title":"Super Resolution"},{"location":"models/super_resolution/#super-resolution","text":"Original image and upscaled image Warning Upscaling takes very long time (approximately 45 seconds per image).","title":"Super Resolution"},{"location":"models/super_resolution/#biwakorealesrgan","text":"Super Resolution model. Attributes: Name Type Description model_path str Path to the model. If automatic download is enabled, it will be downloaded to this path. session rt.InferenceSession ONNX Runtime session. w, h (int Width and height of the model input. input_name str Name of the input node. output_name str Name of the output node.","title":"BiWAKO.RealESRGAN"},{"location":"models/super_resolution/#BiWAKO.model.super_resolution.RealESRGAN.__init__","text":"Initialize RealESRGAN. Parameters: Name Type Description Default model str Model name or path to the downloaded onnx file. If model has not been downloaded, it will be downloaded automatically. Currently supports [\"super_resolution4864\", \"super_resolution6464\"]. 'super_resolution4864'","title":"__init__()"},{"location":"models/super_resolution/#BiWAKO.model.super_resolution.RealESRGAN.predict","text":"Return the upscaled image. Parameters: Name Type Description Default image Image Image to be upscaled. Accept path or cv2 image. required Returns: Type Description np.ndarray Upscaled image in cv2 format.","title":"predict()"},{"location":"models/super_resolution/#BiWAKO.model.super_resolution.RealESRGAN.render","text":"Return the upscaled image. This is just a placeholder. Parameters: Name Type Description Default prediction np.ndarray Upscaled image in cv2 format. This image is returned. required image Image Original image. Not used. required Returns: Type Description np.ndarray Upscaled image in cv2 format.","title":"render()"}]}